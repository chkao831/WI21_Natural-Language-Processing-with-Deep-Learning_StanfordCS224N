% Here is a LaTeX template for CS224n homework.

\documentclass{article}
\usepackage{fullpage,graphicx}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{algorithmic}
\usepackage{minted}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

\begin{document}
\noindent
Carolyn Kao (chkao831@stanford.edu) \hfill \textbf{CS224n Assignment 3} \newline 
February 2, 2021

\noindent
\rule{\linewidth}{0.4pt}


\section*{1) Machine Learning \& Neural Networks }

\begin{enumerate}[label=\alph*)]
\item \hfill \break
i) By keeping track of \textbf{m}, which is a rolling average of the gradients, we basically capture the previous update \textbf{$m_{i-1}$}, integrate that with other gradient components to some degree $\beta_1 \in [0,1]$, and then smooth transitions from one state to another by controlling the hyperparameter $\beta_1$ on the previous state. This allows us to place different weights on different gradient points at position $\theta$ over time such that the dimensions whose gradients point in the same directions is increased while decreasing the updates for dimensions whose gradients change directions. With such smoothing transitions, we are able to lower the variance (mainly because only a (1-$\beta_1$) proportion of \textbf{m} gets to be updated at each iteration, preventing from varying too much) with reduced oscillation and faster convergence , which is optimal to the overall learning; this results from the fact that decreasing the oscillations straightens out the trail that descends to the local optimum, yielding more efficiency. 

ii) As stated, now we have the rolling average of the magnitudes of the gradients $\textbf{v}$, so the model parameters with corresponding smaller gradients (i.e. smaller $\textbf{v}$) would get bigger updates. Dividing the term by $\sqrt{v}$, for parameters with smaller gradients, we basically cut off more (with a smaller divisor) during an update. This allows us to get effective update for stagnant parameters at flat areas by having them move faster along axes and accelerate the learning and convergence. 
\item \hfill \break
i) Mathematically speaking, as $d_{i} \sim Bernoulli(1-p_{drop})$, we could rewrite the given equation as,
$$\begin{aligned} \mathbb{E}_{p_{drop}}[\mathbf{h}_{drop}]_{i} &=  \gamma \mathbb{E}_{p_{drop}}[d_{i}h_{i}]  &= \gamma h(1-p_{drop}) &= h \end{aligned} $$
This gives us $\gamma = \frac{1}{1-p_{drop}}$ during the test phase. 

ii) On the one hand, We should apply dropout during training because we'd like to reduce overfitting with a more generalizable network. With its  randomness, dropout basically reduces the capacity and "thin" the network during training, allowing our model to avoid breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust. \\
On the other hand, we want to evaluate the integrated representational ability of all hidden units during evaluation time. If using "thinned" network, we would not be able to utilize the capability of each learned neurons by skipping some of them randomly, as we haven't been sure which neurons should be laid off with such underlying randomness. Hence, we need to ensemble those hidden units at this stage for examining the generalization power of the model. 

\end{enumerate}
\newpage
\section*{2) Neural Transition-Based Dependency Parsing }
(a)
%\begin{enumerate}[label=\alph*)]
%\item 

%\hfill 
%\break 
\begin{table}[h!]
\centering
% \caption{My caption}
\label{my-label}
\begin{tabular}{@{}llll@{}}
\toprule
Stack & Buffer & New Dependency & Transition \\ 
\midrule $[ROOT]$ & $[I, parsed, this, sentence, correctly]$ & & Initial Configuration \\ 
\midrule $[ROOT, I]$ & $[parsed, this, sentence, correctly]$ & & SHIFT \\ 
\midrule
$[ROOT, I, parsed]$ & $[this, sentence, correctly]$ & & SHIFT \\ 
\midrule
$[ROOT, parsed]$ & $[this, sentence, correctly]$ & $parsed \rightarrow I$ & LEFT-ARC\\
\midrule
$[ROOT, parsed, this]$ & $[sentence, correctly]$ &  & SHIFT\\
\midrule
$[ROOT, parsed, this, sentence]$ & $[correctly]$ &  & SHIFT\\
\midrule
$[ROOT, parsed, sentence]$ & $[correctly]$ & $sentence \rightarrow this$ & LEFT-ARC\\
\midrule
$[ROOT, parsed]$ & $[correctly]$ & $parsed \rightarrow sentence$ & RIGHT-ARC\\
\midrule
$[ROOT, parsed, correctly]$ & $[]$ &  & SHIFT\\
\midrule
$[ROOT, parsed]$ & $[]$ & $parsed \rightarrow correctly$ & RIGHT-ARC\\
\midrule
$[ROOT]$ & $[]$ & $ROOT \rightarrow parsed$ & RIGHT-ARC\\
\bottomrule
\end{tabular}
\end{table}
%\item
(b)
For a sentence containing n words to be parsed, it would roughly take linear time $O(n)$. At each step, we would have two possible state transitions, either to shift word from buffer to stack or to remove a dependent from the stack. Hence, for each word to be shifted, it takes a single step; for a word to be "arc"-ed over from the stack as a dependent, it's also a single step. Specifically speaking, we need $2n$ steps to do so. \\

(e) 

The best UAS (Unlabeled Attachment Score) my model achieves on the dev set is 88.86; the UAS my model achieves on the test set is 88.95. \\
\includegraphics[width=\textwidth]{training_results.png}\\

(f) \\
(i) \textbf{I disembarked and was heading to a wedding fearing my death.}\\
\textbf{Error type}: Verb Phrase Attachment Error\\
\textbf{Incorrect dependency}: $wedding \rightarrow fearing$\\
\textbf{Correct dependency}: $heading \rightarrow fearing$\\
(ii) \textbf{It makes me want to rush out and rescue people from dilemmas of their own making.}\\
\textbf{Error type}: Coordination Attachment Error\\
\textbf{Incorrect dependency}: $makes \rightarrow rescue$\\
\textbf{Correct dependency}: $rush \rightarrow rescue$\\
(iii) \textbf{It is on loan from a guy named Joe O'Neill in Midland, Texas.}\\
\textbf{Error type}: Prepositional Phrase Attachment Error\\
\textbf{Incorrect dependency}: $named \rightarrow Midland$\\
\textbf{Correct dependency}: $guy \rightarrow Midland$\\
(iv) \textbf{Brian has been one of the most crucial elements to the success of Mozilla software.}\\
\textbf{Error type}: Modifier Attachment Error\\
\textbf{Incorrect dependency}: $elements \rightarrow most$\\
\textbf{Correct dependency}: $crucial \rightarrow most$\\

%\end{enumerate}


\end{document}